{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296af473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import svds\n",
    "import gensim\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399c9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = pd.read_csv('../data/complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71385e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replacing the X's with an empty space.\n",
    "# for i in range(0, len(complaints)):\n",
    "#     complaints.loc[i, 'Consumer complaint narrative'] = (\n",
    "#         re.sub('X{2,}', '', complaints.loc[i, 'Consumer complaint narrative'])\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34422cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints['Consumer complaint narrative'] = complaints['Consumer complaint narrative'].str.replace('X{2,}', '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba5266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Train/Test spilt stratifying by Issue.\n",
    "X = complaints[['Consumer complaint narrative']]\n",
    "y = complaints['Issue']\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, random_state = 777, stratify = y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24546ac4",
   "metadata": {},
   "source": [
    "**Looking at a Count Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3087655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the count vectorizer to training data then transforming both the training and test data.\n",
    "vect = CountVectorizer()\n",
    "\n",
    "XTrainVec = vect.fit_transform(xTrain['Consumer complaint narrative'])\n",
    "XTestVec = vect.transform(xTest['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3257187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8713868580094615\n",
      "Confusion Matrix:  [[12223   788   118  5071    91]\n",
      " [  933  3987    24   336    31]\n",
      " [  158    29  2630   251    19]\n",
      " [ 2683   105   102 54258   178]\n",
      " [   92    54    22   279  3896]]\n"
     ]
    }
   ],
   "source": [
    "# Checking the results of a logistic regression model.\n",
    "logreg = LogisticRegression(max_iter = 1000).fit(XTrainVec, yTrain)\n",
    "\n",
    "yPred = logreg.predict(XTestVec)\n",
    "\n",
    "print('Accuracy Score: ', accuracy_score(yTest, yPred))\n",
    "\n",
    "print('Confusion Matrix: ', confusion_matrix(yTest, yPred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88379d94",
   "metadata": {},
   "source": [
    "**Looking into dealing with class imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f802028",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler()\n",
    "\n",
    "xTrainOver, yTrainOver = oversample.fit_resample(\n",
    "    xTrain, \n",
    "    yTrain\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22804aa2",
   "metadata": {},
   "source": [
    "**Count Vectorizer into a Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e870a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the count vectorizer to training data then transforming both the training and test data.\n",
    "vect = CountVectorizer()\n",
    "\n",
    "XTrainVec = vect.fit_transform(xTrainOver['Consumer complaint narrative'])\n",
    "XTestVec = vect.transform(xTest['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95139144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8465107856673986\n",
      "Confusion Matrix:  [[13979  1390   234  2561   127]\n",
      " [  780  4317    46   120    48]\n",
      " [  166    47  2710   126    38]\n",
      " [ 6081   571   461 49829   384]\n",
      " [   99    72    35   176  3961]]\n"
     ]
    }
   ],
   "source": [
    "# Checking the results of a logistic regression model.\n",
    "logreg = LogisticRegression(max_iter = 1000).fit(XTrainVec, yTrainOver)\n",
    "\n",
    "yPred = logreg.predict(XTestVec)\n",
    "\n",
    "print('Accuracy Score: ', accuracy_score(yTest, yPred))\n",
    "\n",
    "print('Confusion Matrix: ', confusion_matrix(yTest, yPred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4aee65f2",
   "metadata": {},
   "source": [
    "**Linear SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b6d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "388a7abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8463410217524163\n",
      "Confusion Matrix:  [[13762  1359   230  2803   137]\n",
      " [  833  4177    57   201    43]\n",
      " [  181    64  2620   186    36]\n",
      " [ 5726   630   332 50317   321]\n",
      " [  115    81    33   209  3905]]\n"
     ]
    }
   ],
   "source": [
    "# Checking the results of a linear svc model.\n",
    "svc = LinearSVC(max_iter = 1000).fit(XTrainVec, yTrainOver)\n",
    "\n",
    "yPred = svc.predict(XTestVec)\n",
    "\n",
    "print('Accuracy Score: ', accuracy_score(yTest, yPred))\n",
    "\n",
    "print('Confusion Matrix: ', confusion_matrix(yTest, yPred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b825f847",
   "metadata": {},
   "source": [
    "**Trying to add class weights instead of oversamplng and switching to tfidf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1025ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Train/Test spilt stratifying by Issue.\n",
    "X = complaints[['Consumer complaint narrative']]\n",
    "y = complaints['Issue']\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, random_state = 777, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1a0b4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "86014ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the tfidf vectorizer to training data then transforming both the training and test data.\n",
    "vect = TfidfVectorizer(stop_words = 'english', ngram_range = (1,2))\n",
    "\n",
    "xTrainVec = vect.fit_transform(xTrain['Consumer complaint narrative'])\n",
    "xTestVec = vect.transform(xTest['Consumer complaint narrative'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f64f1cd",
   "metadata": {},
   "source": [
    "**Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "02997dcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Checking the results of a logistic regression model.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m logreg \u001b[39m=\u001b[39m LogisticRegression(max_iter \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m, class_weight \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mbalanced\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(xTrainVec, yTrain)\n\u001b[0;32m      4\u001b[0m yPred \u001b[39m=\u001b[39m logreg\u001b[39m.\u001b[39mpredict(xTestVec)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy Score: \u001b[39m\u001b[39m'\u001b[39m, accuracy_score(yTest, yPred))\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1291\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1289\u001b[0m     n_threads \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1291\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, prefer\u001b[39m=\u001b[39;49mprefer)(\n\u001b[0;32m   1292\u001b[0m     path_func(\n\u001b[0;32m   1293\u001b[0m         X,\n\u001b[0;32m   1294\u001b[0m         y,\n\u001b[0;32m   1295\u001b[0m         pos_class\u001b[39m=\u001b[39;49mclass_,\n\u001b[0;32m   1296\u001b[0m         Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[0;32m   1297\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[0;32m   1298\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[0;32m   1299\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m   1300\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m   1301\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[0;32m   1302\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[0;32m   1303\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m   1304\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m   1305\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1306\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[0;32m   1307\u001b[0m         coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[0;32m   1308\u001b[0m         penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[0;32m   1309\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[0;32m   1310\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1311\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[0;32m   1312\u001b[0m     )\n\u001b[0;32m   1313\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef)\n\u001b[0;32m   1314\u001b[0m )\n\u001b[0;32m   1316\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:450\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    446\u001b[0m l2_reg_strength \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C\n\u001b[0;32m    447\u001b[0m iprint \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m101\u001b[39m][\n\u001b[0;32m    448\u001b[0m     np\u001b[39m.\u001b[39msearchsorted(np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]), verbose)\n\u001b[0;32m    449\u001b[0m ]\n\u001b[1;32m--> 450\u001b[0m opt_res \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[0;32m    451\u001b[0m     func,\n\u001b[0;32m    452\u001b[0m     w0,\n\u001b[0;32m    453\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    454\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    455\u001b[0m     args\u001b[39m=\u001b[39;49m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[0;32m    456\u001b[0m     options\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint, \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: tol, \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_iter},\n\u001b[0;32m    457\u001b[0m )\n\u001b[0;32m    458\u001b[0m n_iter_i \u001b[39m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    459\u001b[0m     solver,\n\u001b[0;32m    460\u001b[0m     opt_res,\n\u001b[0;32m    461\u001b[0m     max_iter,\n\u001b[0;32m    462\u001b[0m     extra_warning_msg\u001b[39m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    463\u001b[0m )\n\u001b[0;32m    464\u001b[0m w0, loss \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    693\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    694\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    695\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    697\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    700\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:289\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    286\u001b[0m bounds \u001b[39m=\u001b[39m [(\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m l \u001b[39m==\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf \u001b[39melse\u001b[39;00m l, \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m u \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39minf \u001b[39melse\u001b[39;00m u) \u001b[39mfor\u001b[39;00m l, u \u001b[39min\u001b[39;00m bounds]\n\u001b[0;32m    287\u001b[0m \u001b[39m# LBFGSB is sent 'old-style' bounds, 'new-style' bounds are required by\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# approx_derivative and ScalarFunction\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m new_bounds \u001b[39m=\u001b[39m old_bound_to_new(bounds)\n\u001b[0;32m    291\u001b[0m \u001b[39m# check bounds\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39mif\u001b[39;00m (new_bounds[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m new_bounds[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39many():\n",
      "File \u001b[1;32mc:\\Users\\hcgre\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_constraints.py:416\u001b[0m, in \u001b[0;36mold_bound_to_new\u001b[1;34m(bounds)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mold_bound_to_new\u001b[39m(bounds):\n\u001b[0;32m    408\u001b[0m     \u001b[39m\"\"\"Convert the old bounds representation to the new one.\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \n\u001b[0;32m    410\u001b[0m \u001b[39m    The new representation is a tuple (lb, ub) and the old one is a list\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39m    -np.inf/np.inf.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 416\u001b[0m     lb, ub \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbounds)\n\u001b[0;32m    418\u001b[0m     \u001b[39m# Convert occurrences of None to -inf or inf, and replace occurrences of\u001b[39;00m\n\u001b[0;32m    419\u001b[0m     \u001b[39m# any numpy array x with x.item(). Then wrap the results in numpy arrays.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m     lb \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mfloat\u001b[39m(_arr_to_scalar(x)) \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n\u001b[0;32m    421\u001b[0m                    \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m lb])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Checking the results of a logistic regression model.\n",
    "logreg = LogisticRegression(max_iter = 1000, class_weight = 'balanced').fit(xTrainVec, yTrain)\n",
    "\n",
    "yPred = logreg.predict(xTestVec)\n",
    "\n",
    "print('Accuracy Score: ', accuracy_score(yTest, yPred))\n",
    "\n",
    "print('Confusion Matrix: ', confusion_matrix(yTest, yPred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b773b67",
   "metadata": {},
   "source": [
    "**Linear SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fefbfc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9033590619977817\n",
      "Confusion Matrix:  [[14548   732   145  2751   115]\n",
      " [  832  4256    24   174    25]\n",
      " [  110    19  2791   155    12]\n",
      " [ 2862    93    79 54066   226]\n",
      " [   41    36    13    95  4158]]\n"
     ]
    }
   ],
   "source": [
    "# Checking the results of a linear svc model.\n",
    "svc = LinearSVC(max_iter = 1000, class_weight = 'balanced').fit(xTrainVec, yTrain)\n",
    "\n",
    "yPred = svc.predict(xTestVec)\n",
    "\n",
    "print('Accuracy Score: ', accuracy_score(yTest, yPred))\n",
    "\n",
    "print('Confusion Matrix: ', confusion_matrix(yTest, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7359530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain['actual'] = yTrain\n",
    "\n",
    "xTrain_2 = xTrain.loc[xTrain['actual'].isin(['Incorrect information on your report', 'Attempts to collect debt not owed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fd161ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainVec_2 = vect.transform(xTrain_2['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3b63bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest['prediciton'] = yPred\n",
    "\n",
    "xTest_2 = xTest.loc[xTest['prediciton'].isin(['Incorrect information on your report', 'Attempts to collect debt not owed'])]\n",
    "\n",
    "yTest_2 = yTest.loc[xTest['prediciton'].isin(['Incorrect information on your report', 'Attempts to collect debt not owed'])]\n",
    "\n",
    "xTestVec_2 = vect.transform(xTest_2['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a9c2d9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9043287410423884\n",
      "Confusion Matrix:  [[14856     0     0  2443     0]\n",
      " [  854     0     0   152     0]\n",
      " [  116     0     0   149     0]\n",
      " [ 3386     0     0 53542     0]\n",
      " [   40     0     0    96     0]]\n"
     ]
    }
   ],
   "source": [
    "# Checking the results of a linear svc model.\n",
    "svc = LinearSVC(max_iter = 1000, class_weight = 'balanced').fit(xTrainVec_2, xTrain_2['actual'])\n",
    "\n",
    "yPred = svc.predict(xTestVec_2)\n",
    "\n",
    "print('Accuracy Score: ', accuracy_score(yTest_2, yPred))\n",
    "\n",
    "print('Confusion Matrix: ', confusion_matrix(yTest_2, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a524dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9858982774621426\n",
      "Confusion Matrix:  [[ 53645    106     17   1092     12]\n",
      " [    24  15896      0     12      0]\n",
      " [     0      0   9249     11      0]\n",
      " [  2303     56     28 169519     73]\n",
      " [     2      0      0      2  13027]]\n"
     ]
    }
   ],
   "source": [
    "yPred = svc.predict(xTrainVec)\n",
    "\n",
    "print('Accuracy Score: ', accuracy_score(yTrain, yPred))\n",
    "\n",
    "print('Confusion Matrix: ', confusion_matrix(yTrain, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "eaf758c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain['prediciton'] = yPred\n",
    "xTrain['actual'] = yTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1b396630",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_info_wrong = xTrain.loc[\n",
    "    (xTrain['actual'] == 'Incorrect information on your report') & \n",
    "    (xTrain['prediciton'] == 'Attempts to collect debt not owed')\n",
    "    ]\n",
    "\n",
    "attemp_collect_debt_wrong = xTrain.loc[\n",
    "    (xTrain['actual'] == 'Attempts to collect debt not owed') & \n",
    "    (xTrain['prediciton'] == 'Incorrect information on your report')\n",
    "    ]\n",
    "\n",
    "incorrect_info = xTrain.loc[\n",
    "    (xTrain['actual'] == 'Incorrect information on your report') & \n",
    "    (xTrain['prediciton'] == 'Incorrect information on your report')\n",
    "    ]\n",
    "\n",
    "attemp_collect_debt = xTrain.loc[\n",
    "    (xTrain['actual'] == 'Attempts to collect debt not owed') & \n",
    "    (xTrain['prediciton'] == 'Attempts to collect debt not owed')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "33337ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# select the issues with shared words and combine into a single corpus\n",
    "corpus = incorrect_info_wrong['Consumer complaint narrative'].tolist() + attemp_collect_debt['Consumer complaint narrative'].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "word_frequency_dict = {}\n",
    "\n",
    "# process the corpus in chunks to calculate word frequencies incrementally\n",
    "chunk_size = 1000\n",
    "num_samples = len(corpus)\n",
    "\n",
    "for i in range(0, num_samples, chunk_size):\n",
    "    chunk = corpus[i:i+chunk_size]\n",
    "    X = vectorizer.fit_transform(chunk)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    word_frequencies = X.toarray().sum(axis=0)\n",
    "    \n",
    "    # update the word frequency dictionary\n",
    "    for word, frequency in zip(feature_names, word_frequencies):\n",
    "        word_frequency_dict[word] = word_frequency_dict.get(word, 0) + frequency\n",
    "\n",
    "# sort the dictionary by frequency in descending order\n",
    "sorted_words = sorted(word_frequency_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_words = sorted_words[:200]\n",
    "remove_words_attempt_collect_debt = []\n",
    "for word, frequency in top_words:\n",
    "    remove_words_attempt_collect_debt.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f014641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the issues with shared words and combine into a single corpus\n",
    "corpus = incorrect_info_wrong['Consumer complaint narrative'].tolist() + incorrect_info['Consumer complaint narrative'].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "word_frequency_dict = {}\n",
    "\n",
    "# process the corpus in chunks to calculate word frequencies incrementally\n",
    "chunk_size = 1000\n",
    "num_samples = len(corpus)\n",
    "\n",
    "for i in range(0, num_samples, chunk_size):\n",
    "    chunk = corpus[i:i+chunk_size]\n",
    "    X = vectorizer.fit_transform(chunk)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    word_frequencies = X.toarray().sum(axis=0)\n",
    "    \n",
    "    # update the word frequency dictionary\n",
    "    for word, frequency in zip(feature_names, word_frequencies):\n",
    "        word_frequency_dict[word] = word_frequency_dict.get(word, 0) + frequency\n",
    "\n",
    "# sort the dictionary by frequency in descending order\n",
    "sorted_words = sorted(word_frequency_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_words = sorted_words[:]\n",
    "words_incorrect_info = []\n",
    "for word, frequency in top_words:\n",
    "    words_incorrect_info.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "94da9ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words_attempt_collect_debt = [word for word in remove_words_attempt_collect_debt if word not in words_incorrect_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6adea9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove_words_attempt_collect_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9eeea2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# select the issues with shared words and combine into a single corpus\n",
    "corpus = attemp_collect_debt_wrong['Consumer complaint narrative'].tolist() + incorrect_info['Consumer complaint narrative'].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "word_frequency_dict = {}\n",
    "\n",
    "# process the corpus in chunks to calculate word frequencies incrementally\n",
    "chunk_size = 1000\n",
    "num_samples = len(corpus)\n",
    "\n",
    "for i in range(0, num_samples, chunk_size):\n",
    "    chunk = corpus[i:i+chunk_size]\n",
    "    X = vectorizer.fit_transform(chunk)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    word_frequencies = X.toarray().sum(axis=0)\n",
    "    \n",
    "    # update the word frequency dictionary\n",
    "    for word, frequency in zip(feature_names, word_frequencies):\n",
    "        word_frequency_dict[word] = word_frequency_dict.get(word, 0) + frequency\n",
    "\n",
    "# sort the dictionary by frequency in descending order\n",
    "sorted_words = sorted(word_frequency_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_words = sorted_words[:200]\n",
    "remove_incorrect_info = []\n",
    "for word, frequency in top_words:\n",
    "    remove_incorrect_info.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f3ee21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the issues with shared words and combine into a single corpus\n",
    "corpus = attemp_collect_debt_wrong['Consumer complaint narrative'].tolist() + attemp_collect_debt['Consumer complaint narrative'].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "word_frequency_dict = {}\n",
    "\n",
    "# process the corpus in chunks to calculate word frequencies incrementally\n",
    "chunk_size = 1000\n",
    "num_samples = len(corpus)\n",
    "\n",
    "for i in range(0, num_samples, chunk_size):\n",
    "    chunk = corpus[i:i+chunk_size]\n",
    "    X = vectorizer.fit_transform(chunk)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    word_frequencies = X.toarray().sum(axis=0)\n",
    "    \n",
    "    # update the word frequency dictionary\n",
    "    for word, frequency in zip(feature_names, word_frequencies):\n",
    "        word_frequency_dict[word] = word_frequency_dict.get(word, 0) + frequency\n",
    "\n",
    "# sort the dictionary by frequency in descending order\n",
    "sorted_words = sorted(word_frequency_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_words = sorted_words[:]\n",
    "words_attempt_to_collect_dedt = []\n",
    "for word, frequency in top_words:\n",
    "    words_attempt_to_collect_dedt.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_incorrect_info = [word for word in remove_incorrect_info if word not in words_attempt_to_collect_dedt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70e3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove_incorrect_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the words\n",
    "mask = xTrain['prediciton'] == 'Incorrect information on your report'\n",
    "xTrain.loc[mask, 'Consumer complaint narrative'] = xTrain.loc[mask, 'Consumer complaint narrative'].apply(\n",
    "    lambda text: ' '.join([word for word in text.split() if word not in remove_words_attempt_collect_debt])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ae8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the words\n",
    "mask = xTrain['prediciton'] == 'Attempts to collect debt not owed'\n",
    "xTrain.loc[mask, 'Consumer complaint narrative'] = xTrain.loc[mask, 'Consumer complaint narrative'].apply(\n",
    "    lambda text: ' '.join([word for word in text.split() if word not in remove_incorrect_info])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainVec = vect.transform(xTrain['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475daa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9858982774621426\n",
      "Confusion Matrix:  [[ 53645    106     17   1092     12]\n",
      " [    24  15896      0     12      0]\n",
      " [     0      0   9249     11      0]\n",
      " [  2303     56     28 169519     73]\n",
      " [     2      0      0      2  13027]]\n"
     ]
    }
   ],
   "source": [
    "yPred = svc.predict(xTrainVec)\n",
    "\n",
    "print('Accuracy Score: ', accuracy_score(yTrain, yPred))\n",
    "\n",
    "print('Confusion Matrix: ', confusion_matrix(yTrain, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb9a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9033590619977817\n",
      "Confusion Matrix:  [[14548   732   145  2751   115]\n",
      " [  832  4256    24   174    25]\n",
      " [  110    19  2791   155    12]\n",
      " [ 2862    93    79 54066   226]\n",
      " [   41    36    13    95  4158]]\n"
     ]
    }
   ],
   "source": [
    "yPred = svc.predict(xTestVec)\n",
    "\n",
    "print('Accuracy Score: ', accuracy_score(yTest, yPred))\n",
    "\n",
    "print('Confusion Matrix: ', confusion_matrix(yTest, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest['prediciton'] = yPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the words\n",
    "mask = xTest['prediciton'] == 'Incorrect information on your report'\n",
    "xTest.loc[mask, 'Consumer complaint narrative'] = xTest.loc[mask, 'Consumer complaint narrative'].apply(\n",
    "    lambda text: ' '.join([word for word in text.split() if word not in remove_words_attempt_collect_debt])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd46025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the words\n",
    "mask = xTest['prediciton'] == 'Attempts to collect debt not owed'\n",
    "xTest.loc[mask, 'Consumer complaint narrative'] = xTest.loc[mask, 'Consumer complaint narrative'].apply(\n",
    "    lambda text: ' '.join([word for word in text.split() if word not in remove_incorrect_info])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTestVec = vect.transform(xTest['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c114f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9029063582244958\n",
      "Confusion Matrix:  [[14503   757   154  2760   117]\n",
      " [  814  4275    24   173    25]\n",
      " [  107    19  2795   153    13]\n",
      " [ 2874    98    81 54044   229]\n",
      " [   37    36    13    95  4162]]\n"
     ]
    }
   ],
   "source": [
    "yPred = svc.predict(xTestVec)\n",
    "\n",
    "print('Accuracy Score: ', accuracy_score(yTest, yPred))\n",
    "\n",
    "print('Confusion Matrix: ', confusion_matrix(yTest, yPred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
